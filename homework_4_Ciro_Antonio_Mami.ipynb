{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from scripts import mnist\n",
    "from scripts.train_utils import accuracy, AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 1000),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=1000), # we specify the dimensionality of the incoming data\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=500),\n",
    "            nn.Linear(500, 250),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            #nn.BatchNorm1d(num_features=64),\n",
    "            #nn.Linear(64, 64),\n",
    "            #nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=250),\n",
    "            nn.Linear(250, 100),\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "            nn.BatchNorm1d(num_features=100),\n",
    "            nn.Linear(100, 10),\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_and_gradients_norm(named_parameters):\n",
    "    square_norms_params = []\n",
    "    square_norms_grads = []\n",
    "\n",
    "    for _, param in named_parameters:\n",
    "\n",
    "        # Q: what is this and why did I write it here?\n",
    "        if param.requires_grad:\n",
    "            square_norms_params.append((param ** 2).sum())\n",
    "            square_norms_grads.append((param.grad ** 2).sum())\n",
    "    \n",
    "    norm_params = sum(square_norms_params).sqrt().item()\n",
    "    norm_grads = sum(square_norms_grads).sqrt().item()\n",
    "\n",
    "    return norm_params, norm_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance): # note: I've added a generic performance to replace accuracy\n",
    "    for X, y in dataloader:\n",
    "        # 1. reset the gradients previously accumulated by the optimizer\n",
    "        #    this will avoid re-using gradients from previous loops\n",
    "        optimizer.zero_grad() \n",
    "        # 2. get the predictions from the current state of the model\n",
    "        #    this is the forward pass\n",
    "        y_hat = model(X)\n",
    "        # 3. calculate the loss on the current mini-batch\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        # 4. execute the backward pass given the current loss\n",
    "        loss.backward()\n",
    "        # 5. update the value of the params\n",
    "        optimizer.step()\n",
    "        # 6. calculate the accuracy for this mini-batch\n",
    "        acc = performance(y_hat, y)\n",
    "        # 7. update the loss and accuracy AverageMeter\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])\n",
    "\n",
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, lr_scheduler=None, epoch_start_scheduler=1):\n",
    "    # added lr_scheduler\n",
    "\n",
    "    # create the folder for the checkpoints (if it's not None)\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        # added print for LR\n",
    "        print(f\"Epoch {epoch+1} --- learning rate {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        # produce checkpoint dictionary -- but only if the name and folder of the checkpoint are not None\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            if epoch >= epoch_start_scheduler:\n",
    "                lr_scheduler.step()\n",
    "            # or you can use a MultiStepLR with milestones=[6, 11] thus deleting the `if` construct for the epoch\n",
    "        if performance_meter.avg == 1:\n",
    "            return loss_meter.sum, performance_meter.avg\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg\n",
    "\n",
    "def test_model(model, dataloader, performance=accuracy, loss_fn=None, device=None):\n",
    "    # establish device\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # create an AverageMeter for the loss if passed\n",
    "    if loss_fn is not None:\n",
    "        loss_meter = AverageMeter()\n",
    "    \n",
    "    performance_meter = AverageMeter()\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = model(X)\n",
    "            loss = loss_fn(y_hat, y) if loss_fn is not None else None\n",
    "            acc = performance(y_hat, y)\n",
    "            if loss_fn is not None:\n",
    "                loss_meter.update(loss.item(), X.shape[0])\n",
    "            performance_meter.update(acc, X.shape[0])\n",
    "    # get final performances\n",
    "    fin_loss = loss_meter.sum if loss_fn is not None else None\n",
    "    fin_perf = performance_meter.avg\n",
    "    print(f\"TESTING - loss {fin_loss if fin_loss is not None else '--'} - performance {fin_perf}\")\n",
    "    return fin_loss, fin_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size_train = 256\n",
    "minibatch_size_test = 512\n",
    "\n",
    "\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)\n"
   ]
  },
  {
   "source": [
    "In order to reach 100% accuracy on training what I've done has been to enlarge the MLP, adding one layer and specially increasing the number of parameters for each layer. The same model also worked on the permutated labels reaching 99,9% on accuracy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.1\n",
    "\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "best_model = MLP_BN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(best_model.parameters(), lr=learn_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=15)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 14178.19571018219 - average: 0.23630326183636982; Performance: 0.9414\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Epoch 2 completed. Loss - total: 3623.018170952797 - average: 0.060383636182546614; Performance: 0.9847833333333333\n",
      "Epoch 3 --- learning rate 0.09891\n",
      "Epoch 3 completed. Loss - total: 1550.2869215011597 - average: 0.025838115358352662; Performance: 0.9955\n",
      "Epoch 4 --- learning rate 0.09568\n",
      "Epoch 4 completed. Loss - total: 802.5168937146664 - average: 0.013375281561911105; Performance: 0.9984166666666666\n",
      "Epoch 5 --- learning rate 0.09045\n",
      "Epoch 5 completed. Loss - total: 414.4504934847355 - average: 0.006907508224745592; Performance: 0.9996833333333334\n",
      "Epoch 6 --- learning rate 0.08346\n",
      "Epoch 6 completed. Loss - total: 254.04075384140015 - average: 0.0042340125640233355; Performance: 0.9999333333333333\n",
      "Epoch 7 --- learning rate 0.07500\n",
      "Epoch 7 completed. Loss - total: 194.5168101489544 - average: 0.0032419468358159064; Performance: 0.9999666666666667\n",
      "Epoch 8 --- learning rate 0.06545\n",
      "Epoch 8 completed. Loss - total: 162.36983628571033 - average: 0.0027061639380951724; Performance: 0.9999833333333333\n",
      "Epoch 9 --- learning rate 0.05523\n",
      "Epoch 9 completed. Loss - total: 141.86826947331429 - average: 0.0023644711578885715; Performance: 0.9999833333333333\n",
      "Epoch 10 --- learning rate 0.04477\n",
      "Epoch 10 completed. Loss - total: 128.91081503033638 - average: 0.0021485135838389398; Performance: 0.9999833333333333\n",
      "Epoch 11 --- learning rate 0.03455\n",
      "Epoch 11 completed. Loss - total: 119.8120453953743 - average: 0.0019968674232562385; Performance: 1.0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(119.8120453953743, 1.0)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_model(best_model, trainloader, loss_fn, optimizer, num_epochs, lr_scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3333336\n",
      "Epoch 17 --- learning rate 0.09755\n",
      "Epoch 17 completed. Loss - total: 88765.5573310852 - average: 1.4794259555180866; Performance: 0.48291666666666666\n",
      "Epoch 18 --- learning rate 0.09722\n",
      "Epoch 18 completed. Loss - total: 83576.23254776001 - average: 1.3929372091293335; Performance: 0.5158666666666667\n",
      "Epoch 19 --- learning rate 0.09686\n",
      "Epoch 19 completed. Loss - total: 79160.08657455444 - average: 1.319334776242574; Performance: 0.54085\n",
      "Epoch 20 --- learning rate 0.09649\n",
      "Epoch 20 completed. Loss - total: 74688.8642616272 - average: 1.2448144043604532; Performance: 0.5665666666666667\n",
      "Epoch 21 --- learning rate 0.09609\n",
      "Epoch 21 completed. Loss - total: 70150.38455200195 - average: 1.1691730758666992; Performance: 0.5944166666666667\n",
      "Epoch 22 --- learning rate 0.09568\n",
      "Epoch 22 completed. Loss - total: 65413.09572601318 - average: 1.0902182621002197; Performance: 0.6197333333333334\n",
      "Epoch 23 --- learning rate 0.09524\n",
      "Epoch 23 completed. Loss - total: 60915.782329559326 - average: 1.0152630388259887; Performance: 0.6454666666666666\n",
      "Epoch 24 --- learning rate 0.09479\n",
      "Epoch 24 completed. Loss - total: 56597.97482299805 - average: 0.9432995803833008; Performance: 0.67405\n",
      "Epoch 25 --- learning rate 0.09431\n",
      "Epoch 25 completed. Loss - total: 53359.20098876953 - average: 0.8893200164794922; Performance: 0.6928\n",
      "Epoch 26 --- learning rate 0.09382\n",
      "Epoch 26 completed. Loss - total: 49692.891414642334 - average: 0.8282148569107056; Performance: 0.7130833333333333\n",
      "Epoch 27 --- learning rate 0.09330\n",
      "Epoch 27 completed. Loss - total: 46952.67090034485 - average: 0.7825445150057475; Performance: 0.7295166666666667\n",
      "Epoch 28 --- learning rate 0.09277\n",
      "Epoch 28 completed. Loss - total: 43451.831577301025 - average: 0.724197192955017; Performance: 0.75055\n",
      "Epoch 29 --- learning rate 0.09222\n",
      "Epoch 29 completed. Loss - total: 40853.18613433838 - average: 0.6808864355723063; Performance: 0.7655666666666666\n",
      "Epoch 30 --- learning rate 0.09165\n",
      "Epoch 30 completed. Loss - total: 37827.37753677368 - average: 0.6304562922795613; Performance: 0.7811833333333333\n",
      "Epoch 31 --- learning rate 0.09106\n",
      "Epoch 31 completed. Loss - total: 36406.36005592346 - average: 0.6067726675987244; Performance: 0.7896\n",
      "Epoch 32 --- learning rate 0.09045\n",
      "Epoch 32 completed. Loss - total: 33386.263135910034 - average: 0.5564377189318339; Performance: 0.8073333333333333\n",
      "Epoch 33 --- learning rate 0.08983\n",
      "Epoch 33 completed. Loss - total: 31058.42670249939 - average: 0.5176404450416565; Performance: 0.8215833333333333\n",
      "Epoch 34 --- learning rate 0.08918\n",
      "Epoch 34 completed. Loss - total: 28890.371702194214 - average: 0.4815061950365702; Performance: 0.8333833333333334\n",
      "Epoch 35 --- learning rate 0.08853\n",
      "Epoch 35 completed. Loss - total: 26864.621229171753 - average: 0.44774368715286256; Performance: 0.8469\n",
      "Epoch 36 --- learning rate 0.08785\n",
      "Epoch 36 completed. Loss - total: 25662.25531768799 - average: 0.42770425529479983; Performance: 0.8520166666666666\n",
      "Epoch 37 --- learning rate 0.08716\n",
      "Epoch 37 completed. Loss - total: 23699.614645004272 - average: 0.39499357741673785; Performance: 0.8644666666666667\n",
      "Epoch 38 --- learning rate 0.08645\n",
      "Epoch 38 completed. Loss - total: 23351.766609191895 - average: 0.3891961101531982; Performance: 0.8655833333333334\n",
      "Epoch 39 --- learning rate 0.08572\n",
      "Epoch 39 completed. Loss - total: 22758.00398349762 - average: 0.379300066391627; Performance: 0.86865\n",
      "Epoch 40 --- learning rate 0.08498\n",
      "Epoch 40 completed. Loss - total: 21570.902070999146 - average: 0.3595150345166524; Performance: 0.87685\n",
      "Epoch 41 --- learning rate 0.08423\n",
      "Epoch 41 completed. Loss - total: 20953.61625480652 - average: 0.34922693758010864; Performance: 0.8802666666666666\n",
      "Epoch 42 --- learning rate 0.08346\n",
      "Epoch 42 completed. Loss - total: 19384.899678230286 - average: 0.3230816613038381; Performance: 0.8902166666666667\n",
      "Epoch 43 --- learning rate 0.08267\n",
      "Epoch 43 completed. Loss - total: 17180.14345932007 - average: 0.28633572432200116; Performance: 0.9025666666666666\n",
      "Epoch 44 --- learning rate 0.08187\n",
      "Epoch 44 completed. Loss - total: 16085.904207229614 - average: 0.2680984034538269; Performance: 0.9097\n",
      "Epoch 45 --- learning rate 0.08106\n",
      "Epoch 45 completed. Loss - total: 15450.209826469421 - average: 0.25750349710782366; Performance: 0.91335\n",
      "Epoch 46 --- learning rate 0.08023\n",
      "Epoch 46 completed. Loss - total: 14278.296945095062 - average: 0.23797161575158438; Performance: 0.9193166666666667\n",
      "Epoch 47 --- learning rate 0.07939\n",
      "Epoch 47 completed. Loss - total: 13388.386419296265 - average: 0.22313977365493776; Performance: 0.9245\n",
      "Epoch 48 --- learning rate 0.07854\n",
      "Epoch 48 completed. Loss - total: 12919.123661994934 - average: 0.21531872769991556; Performance: 0.9279833333333334\n",
      "Epoch 49 --- learning rate 0.07767\n",
      "Epoch 49 completed. Loss - total: 12476.474355697632 - average: 0.2079412392616272; Performance: 0.92985\n",
      "Epoch 50 --- learning rate 0.07679\n",
      "Epoch 50 completed. Loss - total: 12145.888355731964 - average: 0.20243147259553274; Performance: 0.9314833333333333\n",
      "Epoch 51 --- learning rate 0.07590\n",
      "Epoch 51 completed. Loss - total: 11606.016293525696 - average: 0.19343360489209493; Performance: 0.9348833333333333\n",
      "Epoch 52 --- learning rate 0.07500\n",
      "Epoch 52 completed. Loss - total: 11044.60466003418 - average: 0.184076744333903; Performance: 0.9384833333333333\n",
      "Epoch 53 --- learning rate 0.07409\n",
      "Epoch 53 completed. Loss - total: 10241.849876403809 - average: 0.17069749794006348; Performance: 0.9430833333333334\n",
      "Epoch 54 --- learning rate 0.07316\n",
      "Epoch 54 completed. Loss - total: 9624.532472610474 - average: 0.16040887454350788; Performance: 0.9461666666666667\n",
      "Epoch 55 --- learning rate 0.07223\n",
      "Epoch 55 completed. Loss - total: 9073.923515319824 - average: 0.15123205858866373; Performance: 0.9497333333333333\n",
      "Epoch 56 --- learning rate 0.07129\n",
      "Epoch 56 completed. Loss - total: 8504.915515899658 - average: 0.14174859193166098; Performance: 0.95335\n",
      "Epoch 57 --- learning rate 0.07034\n",
      "Epoch 57 completed. Loss - total: 7923.740390300751 - average: 0.13206233983834584; Performance: 0.9560166666666666\n",
      "Epoch 58 --- learning rate 0.06938\n",
      "Epoch 58 completed. Loss - total: 7664.859579086304 - average: 0.1277476596514384; Performance: 0.9577166666666667\n",
      "Epoch 59 --- learning rate 0.06841\n",
      "Epoch 59 completed. Loss - total: 7264.183577537537 - average: 0.12106972629229228; Performance: 0.9612666666666667\n",
      "Epoch 60 --- learning rate 0.06743\n",
      "Epoch 60 completed. Loss - total: 7057.322562217712 - average: 0.11762204270362854; Performance: 0.9609666666666666\n",
      "Epoch 61 --- learning rate 0.06644\n",
      "Epoch 61 completed. Loss - total: 6799.323395252228 - average: 0.11332205658753713; Performance: 0.9626166666666667\n",
      "Epoch 62 --- learning rate 0.06545\n",
      "Epoch 62 completed. Loss - total: 6252.5546288490295 - average: 0.10420924381415049; Performance: 0.9657\n",
      "Epoch 63 --- learning rate 0.06445\n",
      "Epoch 63 completed. Loss - total: 6117.520616531372 - average: 0.10195867694218953; Performance: 0.9668166666666667\n",
      "Epoch 64 --- learning rate 0.06345\n",
      "Epoch 64 completed. Loss - total: 5799.276693582535 - average: 0.09665461155970892; Performance: 0.9683666666666667\n",
      "Epoch 65 --- learning rate 0.06243\n",
      "Epoch 65 completed. Loss - total: 5238.624851703644 - average: 0.08731041419506073; Performance: 0.9709833333333333\n",
      "Epoch 66 --- learning rate 0.06142\n",
      "Epoch 66 completed. Loss - total: 4722.018461704254 - average: 0.0787003076950709; Performance: 0.9747666666666667\n",
      "Epoch 67 --- learning rate 0.06040\n",
      "Epoch 67 completed. Loss - total: 4328.654602050781 - average: 0.07214424336751302; Performance: 0.9769\n",
      "Epoch 68 --- learning rate 0.05937\n",
      "Epoch 68 completed. Loss - total: 4257.098915338516 - average: 0.07095164858897528; Performance: 0.9774\n",
      "Epoch 69 --- learning rate 0.05834\n",
      "Epoch 69 completed. Loss - total: 3787.6997861862183 - average: 0.0631283297697703; Performance: 0.98025\n",
      "Epoch 70 --- learning rate 0.05730\n",
      "Epoch 70 completed. Loss - total: 3828.0448048114777 - average: 0.06380074674685796; Performance: 0.98005\n",
      "Epoch 71 --- learning rate 0.05627\n",
      "Epoch 71 completed. Loss - total: 3365.5984976291656 - average: 0.05609330829381943; Performance: 0.9828166666666667\n",
      "Epoch 72 --- learning rate 0.05523\n",
      "Epoch 72 completed. Loss - total: 3192.683822631836 - average: 0.053211397043863934; Performance: 0.9839833333333333\n",
      "Epoch 73 --- learning rate 0.05418\n",
      "Epoch 73 completed. Loss - total: 2828.3488669395447 - average: 0.047139147782325745; Performance: 0.9862666666666666\n",
      "Epoch 74 --- learning rate 0.05314\n",
      "Epoch 74 completed. Loss - total: 2680.1700936555862 - average: 0.04466950156092644; Performance: 0.9866833333333334\n",
      "Epoch 75 --- learning rate 0.05209\n",
      "Epoch 75 completed. Loss - total: 2364.299627304077 - average: 0.03940499378840129; Performance: 0.9886166666666667\n",
      "Epoch 76 --- learning rate 0.05105\n",
      "Epoch 76 completed. Loss - total: 2204.4085999131203 - average: 0.036740143331885335; Performance: 0.9895333333333334\n",
      "Epoch 77 --- learning rate 0.05000\n",
      "Epoch 77 completed. Loss - total: 1984.6660513877869 - average: 0.033077767523129784; Performance: 0.9907666666666667\n",
      "Epoch 78 --- learning rate 0.04895\n",
      "Epoch 78 completed. Loss - total: 1810.6972274780273 - average: 0.030178287124633787; Performance: 0.9919333333333333\n",
      "Epoch 79 --- learning rate 0.04791\n",
      "Epoch 79 completed. Loss - total: 1893.864891409874 - average: 0.03156441485683123; Performance: 0.9911166666666666\n",
      "Epoch 80 --- learning rate 0.04686\n",
      "Epoch 80 completed. Loss - total: 1558.8372102975845 - average: 0.02598062017162641; Performance: 0.9933166666666666\n",
      "Epoch 81 --- learning rate 0.04582\n",
      "Epoch 81 completed. Loss - total: 1525.357357621193 - average: 0.025422622627019883; Performance: 0.99365\n",
      "Epoch 82 --- learning rate 0.04477\n",
      "Epoch 82 completed. Loss - total: 1221.3634399175644 - average: 0.020356057331959408; Performance: 0.9951166666666666\n",
      "Epoch 83 --- learning rate 0.04373\n",
      "Epoch 83 completed. Loss - total: 1109.120910525322 - average: 0.018485348508755366; Performance: 0.99605\n",
      "Epoch 84 --- learning rate 0.04270\n",
      "Epoch 84 completed. Loss - total: 1030.0393227934837 - average: 0.01716732204655806; Performance: 0.99615\n",
      "Epoch 85 --- learning rate 0.04166\n",
      "Epoch 85 completed. Loss - total: 923.8640241622925 - average: 0.015397733736038208; Performance: 0.9967666666666667\n",
      "Epoch 86 --- learning rate 0.04063\n",
      "Epoch 86 completed. Loss - total: 918.2783398628235 - average: 0.015304638997713725; Performance: 0.99705\n",
      "Epoch 87 --- learning rate 0.03960\n",
      "Epoch 87 completed. Loss - total: 749.5702047348022 - average: 0.012492836745580037; Performance: 0.99765\n",
      "Epoch 88 --- learning rate 0.03858\n",
      "Epoch 88 completed. Loss - total: 700.7455993294716 - average: 0.01167909332215786; Performance: 0.9979666666666667\n",
      "Epoch 89 --- learning rate 0.03757\n",
      "Epoch 89 completed. Loss - total: 684.3505354523659 - average: 0.011405842257539432; Performance: 0.9979666666666667\n",
      "Epoch 90 --- learning rate 0.03655\n",
      "Epoch 90 completed. Loss - total: 585.397299259901 - average: 0.009756621654331684; Performance: 0.99855\n",
      "Epoch 91 --- learning rate 0.03555\n",
      "Epoch 91 completed. Loss - total: 523.7934774458408 - average: 0.008729891290764014; Performance: 0.9988166666666667\n",
      "Epoch 92 --- learning rate 0.03455\n",
      "Epoch 92 completed. Loss - total: 481.00045508146286 - average: 0.008016674251357714; Performance: 0.9989333333333333\n",
      "Epoch 93 --- learning rate 0.03356\n",
      "Epoch 93 completed. Loss - total: 480.41848835349083 - average: 0.008006974805891513; Performance: 0.99865\n",
      "Epoch 94 --- learning rate 0.03257\n",
      "Epoch 94 completed. Loss - total: 483.090967386961 - average: 0.008051516123116016; Performance: 0.9988166666666667\n",
      "Epoch 95 --- learning rate 0.03159\n",
      "Epoch 95 completed. Loss - total: 427.3545730113983 - average: 0.007122576216856639; Performance: 0.99905\n",
      "Epoch 96 --- learning rate 0.03062\n",
      "Epoch 96 completed. Loss - total: 384.1534408032894 - average: 0.006402557346721491; Performance: 0.9991833333333333\n",
      "Epoch 97 --- learning rate 0.02966\n",
      "Epoch 97 completed. Loss - total: 382.56109204888344 - average: 0.006376018200814724; Performance: 0.9992833333333333\n",
      "Epoch 98 --- learning rate 0.02871\n",
      "Epoch 98 completed. Loss - total: 340.5379940867424 - average: 0.00567563323477904; Performance: 0.99935\n",
      "Epoch 99 --- learning rate 0.02777\n",
      "Epoch 99 completed. Loss - total: 328.39954775571823 - average: 0.0054733257959286375; Performance: 0.9993666666666666\n",
      "Epoch 100 --- learning rate 0.02684\n",
      "Epoch 100 completed. Loss - total: 298.9074386060238 - average: 0.00498179064343373; Performance: 0.99945\n",
      "Epoch 101 --- learning rate 0.02591\n",
      "Epoch 101 completed. Loss - total: 279.7295920550823 - average: 0.004662159867584705; Performance: 0.9996166666666667\n",
      "Epoch 102 --- learning rate 0.02500\n",
      "Epoch 102 completed. Loss - total: 268.2757114470005 - average: 0.004471261857450008; Performance: 0.9996333333333334\n",
      "Epoch 103 --- learning rate 0.02410\n",
      "Epoch 103 completed. Loss - total: 290.38646975159645 - average: 0.004839774495859941; Performance: 0.9994833333333333\n",
      "Epoch 104 --- learning rate 0.02321\n",
      "Epoch 104 completed. Loss - total: 293.8721335530281 - average: 0.004897868892550468; Performance: 0.9994166666666666\n",
      "Epoch 105 --- learning rate 0.02233\n",
      "Epoch 105 completed. Loss - total: 281.76638543605804 - average: 0.004696106423934301; Performance: 0.9996166666666667\n",
      "Epoch 106 --- learning rate 0.02146\n",
      "Epoch 106 completed. Loss - total: 251.99976862967014 - average: 0.004199996143827836; Performance: 0.9997666666666667\n",
      "Epoch 107 --- learning rate 0.02061\n",
      "Epoch 107 completed. Loss - total: 243.30086514353752 - average: 0.0040550144190589586; Performance: 0.99975\n",
      "Epoch 108 --- learning rate 0.01977\n",
      "Epoch 108 completed. Loss - total: 246.73945325613022 - average: 0.004112324220935504; Performance: 0.9996166666666667\n",
      "Epoch 109 --- learning rate 0.01894\n",
      "Epoch 109 completed. Loss - total: 238.7134488672018 - average: 0.00397855748112003; Performance: 0.9996333333333334\n",
      "Epoch 110 --- learning rate 0.01813\n",
      "Epoch 110 completed. Loss - total: 231.18461260199547 - average: 0.0038530768766999244; Performance: 0.99955\n",
      "Epoch 111 --- learning rate 0.01733\n",
      "Epoch 111 completed. Loss - total: 221.4655400812626 - average: 0.00369109233468771; Performance: 0.9997\n",
      "Epoch 112 --- learning rate 0.01654\n",
      "Epoch 112 completed. Loss - total: 195.88178046047688 - average: 0.003264696341007948; Performance: 0.9997166666666667\n",
      "Epoch 113 --- learning rate 0.01577\n",
      "Epoch 113 completed. Loss - total: 201.4251014739275 - average: 0.0033570850245654583; Performance: 0.9996833333333334\n",
      "Epoch 114 --- learning rate 0.01502\n",
      "Epoch 114 completed. Loss - total: 191.89017237722874 - average: 0.0031981695396204788; Performance: 0.9997333333333334\n",
      "Epoch 115 --- learning rate 0.01428\n",
      "Epoch 115 completed. Loss - total: 182.25598323345184 - average: 0.0030375997205575305; Performance: 0.9998\n",
      "Epoch 116 --- learning rate 0.01355\n",
      "Epoch 116 completed. Loss - total: 173.24120876193047 - average: 0.002887353479365508; Performance: 0.9998333333333334\n",
      "Epoch 117 --- learning rate 0.01284\n",
      "Epoch 117 completed. Loss - total: 174.287597194314 - average: 0.0029047932865719; Performance: 0.99985\n",
      "Epoch 118 --- learning rate 0.01215\n",
      "Epoch 118 completed. Loss - total: 191.06272660195827 - average: 0.0031843787766993046; Performance: 0.9997166666666667\n",
      "Epoch 119 --- learning rate 0.01147\n",
      "Epoch 119 completed. Loss - total: 164.04932741820812 - average: 0.002734155456970135; Performance: 0.9998666666666667\n",
      "Epoch 120 --- learning rate 0.01082\n",
      "Epoch 120 completed. Loss - total: 165.48426453769207 - average: 0.002758071075628201; Performance: 0.9998833333333333\n",
      "Epoch 121 --- learning rate 0.01017\n",
      "Epoch 121 completed. Loss - total: 154.58762964606285 - average: 0.0025764604941010474; Performance: 0.9999\n",
      "Epoch 122 --- learning rate 0.00955\n",
      "Epoch 122 completed. Loss - total: 157.36768938601017 - average: 0.0026227948231001695; Performance: 0.9999166666666667\n",
      "Epoch 123 --- learning rate 0.00894\n",
      "Epoch 123 completed. Loss - total: 151.4037330299616 - average: 0.00252339555049936; Performance: 0.9998833333333333\n",
      "Epoch 124 --- learning rate 0.00835\n",
      "Epoch 124 completed. Loss - total: 168.33861704170704 - average: 0.002805643617361784; Performance: 0.9997666666666667\n",
      "Epoch 125 --- learning rate 0.00778\n",
      "Epoch 125 completed. Loss - total: 156.37769626826048 - average: 0.0026062949378043414; Performance: 0.9999166666666667\n",
      "Epoch 126 --- learning rate 0.00723\n",
      "Epoch 126 completed. Loss - total: 157.53872221708298 - average: 0.0026256453702847163; Performance: 0.9997166666666667\n",
      "Epoch 127 --- learning rate 0.00670\n",
      "Epoch 127 completed. Loss - total: 150.89586471021175 - average: 0.0025149310785035294; Performance: 0.9998833333333333\n",
      "Epoch 128 --- learning rate 0.00618\n",
      "Epoch 128 completed. Loss - total: 146.94182814657688 - average: 0.0024490304691096145; Performance: 0.9998833333333333\n",
      "Epoch 129 --- learning rate 0.00569\n",
      "Epoch 129 completed. Loss - total: 149.40854793787003 - average: 0.002490142465631167; Performance: 0.9998666666666667\n",
      "Epoch 130 --- learning rate 0.00521\n",
      "Epoch 130 completed. Loss - total: 156.19988879561424 - average: 0.002603331479926904; Performance: 0.9998333333333334\n",
      "Epoch 131 --- learning rate 0.00476\n",
      "Epoch 131 completed. Loss - total: 159.32950161397457 - average: 0.002655491693566243; Performance: 0.99975\n",
      "Epoch 132 --- learning rate 0.00432\n",
      "Epoch 132 completed. Loss - total: 148.8520566523075 - average: 0.002480867610871792; Performance: 0.9998\n",
      "Epoch 133 --- learning rate 0.00391\n",
      "Epoch 133 completed. Loss - total: 141.8885922729969 - average: 0.002364809871216615; Performance: 0.99985\n",
      "Epoch 134 --- learning rate 0.00351\n",
      "Epoch 134 completed. Loss - total: 143.1235473304987 - average: 0.0023853924555083117; Performance: 0.9998333333333334\n",
      "Epoch 135 --- learning rate 0.00314\n",
      "Epoch 135 completed. Loss - total: 143.5091968625784 - average: 0.00239181994770964; Performance: 0.9998666666666667\n",
      "Epoch 136 --- learning rate 0.00278\n",
      "Epoch 136 completed. Loss - total: 136.8916846960783 - average: 0.0022815280782679716; Performance: 0.9998833333333333\n",
      "Epoch 137 --- learning rate 0.00245\n",
      "Epoch 137 completed. Loss - total: 137.74591886997223 - average: 0.002295765314499537; Performance: 0.9998666666666667\n",
      "Epoch 138 --- learning rate 0.00213\n",
      "Epoch 138 completed. Loss - total: 145.89476846158504 - average: 0.002431579474359751; Performance: 0.9997833333333334\n",
      "Epoch 139 --- learning rate 0.00184\n",
      "Epoch 139 completed. Loss - total: 138.48332151770592 - average: 0.002308055358628432; Performance: 0.9998666666666667\n",
      "Epoch 140 --- learning rate 0.00157\n",
      "Epoch 140 completed. Loss - total: 139.7604800760746 - average: 0.0023293413346012434; Performance: 0.9999333333333333\n",
      "Epoch 141 --- learning rate 0.00132\n",
      "Epoch 141 completed. Loss - total: 138.1242971867323 - average: 0.0023020716197788716; Performance: 0.9998833333333333\n",
      "Epoch 142 --- learning rate 0.00109\n",
      "Epoch 142 completed. Loss - total: 144.3463791757822 - average: 0.0024057729862630365; Performance: 0.9998\n",
      "Epoch 143 --- learning rate 0.00089\n",
      "Epoch 143 completed. Loss - total: 141.08887401223183 - average: 0.0023514812335371973; Performance: 0.9998166666666667\n",
      "Epoch 144 --- learning rate 0.00070\n",
      "Epoch 144 completed. Loss - total: 129.4045801460743 - average: 0.0021567430024345717; Performance: 0.9998666666666667\n",
      "Epoch 145 --- learning rate 0.00054\n",
      "Epoch 145 completed. Loss - total: 134.78670935332775 - average: 0.002246445155888796; Performance: 0.9998666666666667\n",
      "Epoch 146 --- learning rate 0.00039\n",
      "Epoch 146 completed. Loss - total: 142.40478794276714 - average: 0.0023734131323794525; Performance: 0.99985\n",
      "Epoch 147 --- learning rate 0.00027\n",
      "Epoch 147 completed. Loss - total: 149.44303868710995 - average: 0.0024907173114518326; Performance: 0.9996666666666667\n",
      "Epoch 148 --- learning rate 0.00018\n",
      "Epoch 148 completed. Loss - total: 141.16677990555763 - average: 0.0023527796650926274; Performance: 0.9998\n",
      "Epoch 149 --- learning rate 0.00010\n",
      "Epoch 149 completed. Loss - total: 146.15237081050873 - average: 0.0024358728468418123; Performance: 0.9999\n",
      "Epoch 150 --- learning rate 0.00004\n",
      "Epoch 150 completed. Loss - total: 135.00259064137936 - average: 0.0022500431773563225; Performance: 0.9999333333333333\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(135.00259064137936, 0.9999333333333333)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model = MLP_BN()\n",
    "\n",
    "trainset_permuted=trainset\n",
    "trainset_permuted.targets= trainset_permuted.targets[torch.randperm(trainset_permuted.targets.size()[0])]\n",
    "trainloader_permuted = torch.utils.data.DataLoader(trainset_permuted, batch_size=minibatch_size_train, shuffle=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=150)\n",
    "train_model(model, trainloader_permuted, loss_fn, optimizer, 150, lr_scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss -- - performance 0.9998666666666667\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, 0.9998666666666667)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "test_model(model, testloader, performance=accuracy, loss_fn=None, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss -- - performance 0.1013\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, 0.1013)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "test_model(best_model, testloader, performance=accuracy, loss_fn=None, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('dl': conda)",
   "name": "python386jvsc74a57bd027e28fc259bc61cd927bae64e8eebfdf1e667d3e4f2fc6e2557a27161ca9f26d",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "27e28fc259bc61cd927bae64e8eebfdf1e667d3e4f2fc6e2557a27161ca9f26d"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}