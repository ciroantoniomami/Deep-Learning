{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "I've initialized the data generating a tensor of 64 element of dimension 6, half of them will be symmetric and half of them not. I then created another tensor with the label (1 if symmetric 0 if it's not)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from random import randrange\n",
    "data = torch.zeros([64,6])\n",
    "\n",
    "for i in range(64):\n",
    "    for j in range(3):\n",
    "        data[i,j] = randrange(10)\n",
    "\n",
    "for i in range(1,64,2):\n",
    "    for j in range(1,4):\n",
    "        data[i,-j] = data[i,j-1]\n",
    "\n",
    "for i in range(0,64,2):\n",
    "    for j in range(3,6):\n",
    "        data[i,j] = randrange(10)\n",
    "\n",
    "target = torch.Tensor([i%2 for i in range(64)])\n",
    "target = target.reshape(64,1)\n",
    "\n",
    "data= torch.cat((data, target), dim=1)\n",
    "data= data[torch.randperm(data.size()[0])]\n",
    "\n",
    "X=data[0:50,0:6]\n",
    "y=data[0:50,6]\n",
    "y=y.reshape(50,1)\n",
    "X_test=data[50:64,0:6]\n",
    "y_test=data[50:64,6]\n",
    "y_test=y_test.reshape(14,1)\n",
    "\n"
   ]
  },
  {
   "source": [
    "The model is composed by two layer, for the activation function I opted for ReLU, while for the output layer I've used the sigmoid since we are dealing with a binary classification. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(6,2, bias = True)\n",
    "        self.layer2 = torch.nn.Linear(2,1, bias = True)\n",
    "\n",
    "    def forward(self, X): \n",
    "        out = self.layer1(X)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = torch.sigmoid(out) \n",
    "        return out"
   ]
  },
  {
   "source": [
    "I've implemented the training very similar as seen in class, I did not use the dataloader since I'm still not totally certain of how to initialize."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, X,y, loss_fn, optimizer):\n",
    "\n",
    "     optimizer.zero_grad() \n",
    "\n",
    "     y_hat = model(X)\n",
    "\n",
    "\n",
    "     loss = loss_fn(y_hat, y)\n",
    "\n",
    "     loss.backward()\n",
    "\n",
    "     optimizer.step()\n",
    "\n",
    "     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X,y, loss_fn, optimizer, num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        model = train_epoch(model, X,y, loss_fn, optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "source": [
    "As stated in the paper the learning rate is set to 0.1, for the loss function I opted for the binary cross entropy which I thought was the most coherent with the problem, as it was requested I've used the gradient descendent for the optimizer. I've also initialized the weight as suggested from the paper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.1\n",
    "num_epochs = 2000\n",
    "model = MLP()\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "state_dict[\"layer1.weight\"] = (0.3 + 0.3) * torch.rand(2, 6) - 0.3\n",
    "state_dict[\"layer2.weight\"] = (0.3 + 0.3) * torch.rand(1, 2) - 0.3\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " final_model = train_model(model, X,y, loss_fn, optimizer, num_epochs)"
   ]
  },
  {
   "source": [
    "Here are the final weight, we can see that some of them are opposite (as it was in the paper), and also as in the paper the bias are all negative."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-0.7802,  1.9347, -1.0833,  0.9722, -1.8873,  0.8602],\n",
       "                      [ 0.1014,  1.4374, -1.3245, -0.0651, -0.7436,  0.8644]])),\n",
       "             ('layer1.bias', tensor([-0.3103,  0.2813])),\n",
       "             ('layer2.weight', tensor([[-2.6809,  1.6138]])),\n",
       "             ('layer2.bias', tensor([-0.0024]))])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "state_dict = final_model.state_dict()\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(model,X,y):\n",
    "    tmp = model(X)\n",
    "    y_hat = torch.Tensor([(0 if i.item() <= 0.5 else 1)for i in tmp])\n",
    "    y_hat=y_hat.unsqueeze(-1)\n",
    "    match_ground_truth = y_hat == y # -> tensor of booleans\n",
    "    correct_matches = match_ground_truth.sum()\n",
    "    return 1-(correct_matches / y_hat.shape[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.2857142686843872"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "error_rate(final_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}