{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I defined the class for the MultiLayer Perceptron, we have 4 hidden layers and one output layer, for each hidden layer the activation function is the Rectified Linear Unit whereas the activation function for the last layer is softmax, due to the fact the we are facing  multi-class classification problem.\n",
    "\n",
    "import torch\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(in_features=5, out_features=11, bias = False)\n",
    "        self.layer2 = torch.nn.Linear(in_features=11, out_features=16, bias = False)\n",
    "        self.layer3 = torch.nn.Linear(in_features=16, out_features=13, bias = False)\n",
    "        self.layer4 = torch.nn.Linear(in_features=13, out_features=8, bias = False)\n",
    "        self.layer5 = torch.nn.Linear(in_features=8, out_features=4, bias = False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.layer1(X)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer3(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer4(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer5(out)\n",
    "        out = torch.nn.functional.softmax(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I instatiate the model\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Linear: 1-1                            55\n├─Linear: 1-2                            176\n├─Linear: 1-3                            208\n├─Linear: 1-4                            104\n├─Linear: 1-5                            32\n=================================================================\nTotal params: 575\nTrainable params: 575\nNon-trainable params: 0\n=================================================================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Linear: 1-1                            55\n",
       "├─Linear: 1-2                            176\n",
       "├─Linear: 1-3                            208\n",
       "├─Linear: 1-4                            104\n",
       "├─Linear: 1-5                            32\n",
       "=================================================================\n",
       "Total params: 575\n",
       "Trainable params: 575\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "#Here we have the summary of the model, and we can see the number of parameters for each layer-layer couple\n",
    "from torchsummary import summary\n",
    "summary(model)"
   ]
  },
  {
   "source": [
    "The number of parameters for each layer-by-layer couple without bias could be easily computated knowing the number of neurons of the left-side layer and the number of neurons of the right-side layer, then the formulation become:\n",
    "\n",
    "$N_{parameters-layer-by-layer} = N_{neurons-left}*N_{neurons-right}$\n",
    "\n",
    "E.G. $N_{parameters-layer-1-2} = 11 * 16 = 176$\n",
    "\n",
    "If we also have the bias term in the right-side layer the formulation become:\n",
    "\n",
    "$N_{parameters-layer-by-layer} = N_{neurons-left}*N_{neurons-right} + N_{neurons-right}$\n",
    "\n",
    "Indeed for each neuron on the right-side we have a bias term\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\t       L1 Norm  L2 Norm\n",
      "layer1.weight\t12.464\t1.956\n",
      "layer2.weight\t27.184\t2.360\n",
      "layer3.weight\t25.766\t2.053\n",
      "layer4.weight\t15.898\t1.749\n",
      "layer5.weight\t6.120\t1.232\n"
     ]
    }
   ],
   "source": [
    "#Below I printed the L1 and L2 norm for each set of paramaters exploiting the state_dict method and the torch.norm function\n",
    "from sys import stdout\n",
    "stdout.write('\\t       L1 Norm')\n",
    "stdout.write('  L2 Norm')\n",
    "stdout.write('\\n')\n",
    "for param_name, param in model.state_dict().items():\n",
    "    stdout.write(f'{param_name}')\n",
    "    for i in range(1,3):\n",
    "        T = torch.norm(param,i)\n",
    "        stdout.write('\\t{:.3f}'.format(T)) \n",
    "        stdout.flush()\n",
    "    stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}